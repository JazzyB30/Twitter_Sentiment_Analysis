# -*- coding: utf-8 -*-
"""TwitterSentimentAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XBt7G2GeE2v8Z0oeApYhlTig8egAE1Ct

# **Import the Necessary Dependencies**
"""

# utilities
import re
import numpy as np
import pandas as pd
import nltk
import string
# plotting
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from textblob import TextBlob
# nltk
from nltk.stem import WordNetLemmatizer
# sklearn
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import BernoulliNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, classification_report

"""# **Read and Load the Dataset**"""

# Importing the dataset
DATASET_COLUMNS=['id','link','symbol','username','user_id','tweet','retweet_count','likes','reply_count','post_time','views','followers','user_verified','quotecount']
DATASET_ENCODING = "ISO-8859-1"
path="/content/drive/MyDrive/Sentiment Analysis/OutputsTwitter.csv"
df = pd.read_csv(path,encoding=DATASET_ENCODING, names=DATASET_COLUMNS)
df.sample(5)

"""# **Preprocessing the dataset**"""

# removes pattern in the input text
def remove_pattern(input_txt, pattern):
    r = re.findall(pattern, input_txt)
    for word in r:
        input_txt = re.sub(word, "", input_txt)
    return input_txt
df.head()

# Define a function to clean repeating characters
def cleaning_repeating_char(text):
    # Convert input to string
    text = str(text)
    return re.sub(r'(.)\1+', r'\1', text)

# Apply the cleaning function to the 'tweet' column
df['clean_tweet'] = df['tweet'].apply(lambda x: cleaning_repeating_char(x))

# Print the cleaned tweets
df['clean_tweet'].tail()
df.head()

# Define a function to clean URLs
def cleaning_URLs(data):
    if isinstance(data, str):
        return re.sub(r'http\S+', '', data)
    else:
        return ""

# Apply the cleaning function to the 'tweet' column
df['clean_tweet'] = df['tweet'].apply(lambda x: cleaning_URLs(x))

# Print the cleaned tweets
df.head()

# remove twitter handles (@user)
df['clean_tweet'] = np.vectorize(remove_pattern)(df['tweet'], "@[\w]*")
df.head()

#remove stopwords
nltk.download('stopwords')
from nltk.corpus import stopwords
",".join(stopwords.words('english'))
stop_words=set(stopwords.words('english'))

#function to remove stopwords
def remove_stop(x):
  return",".join([word for word in str(x).split() if word not in stop_words])
df['clean_tweet']=df['clean_tweet'].apply(lambda x: remove_stop(x))

#show output
df.head()

# remove special characters, numbers and punctuations
df['clean_tweet'] = df['clean_tweet'].str.replace("[^a-zA-Z#]", " ")
df.head()

# remove short words
df['clean_tweet'] = df['clean_tweet'].apply(lambda x: " ".join([w for w in x.split() if len(w)>3]))
df.head()

# individual words considered as tokens
tokenized_tweet = df['clean_tweet'].apply(lambda x: x.split())
tokenized_tweet.head()

# stem the words
from nltk.stem.porter import PorterStemmer
stemmer = PorterStemmer()

tokenized_tweet = tokenized_tweet.apply(lambda sentence: [stemmer.stem(word) for word in sentence])
tokenized_tweet.head()

# combine words into single sentence
for i in range(len(tokenized_tweet)):
    tokenized_tweet[i] = " ".join(tokenized_tweet[i])
    
df['clean_tweet'] = tokenized_tweet
df.head()

"""# **Subjectivity and Polarity**"""

#create a function to get subjectivity
def getSubjectivity(text):
  return TextBlob(text).sentiment.subjectivity

#create a function to get polarity
def getPolarity(text):
  return TextBlob(text).sentiment.polarity

#create two new columns
df['Subjectivity']=df['clean_tweet'].apply(getSubjectivity)
df['Polarity']=df['clean_tweet'].apply(getPolarity)

#show the dataframe with two new columns
df

"""# **Exploratory Data Analysis**"""

!pip install wordcloud

# visualize the frequent words
all_words = " ".join([sentence for sentence in df['clean_tweet']])

from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)

# plot the graph
plt.figure(figsize=(15,8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

#create a function to compute the negative, positive analysis
def getAnalysis(score):
  if score<0:
    return int(-1)
  elif score==0:
    return int(0)
  
  else:
    return int(1)

df['Analysis']=df['Polarity'].apply(getAnalysis)

#show the dataframe
df

#Selecting the clean_tweet and Analysis column for our further analysis
data=df[['clean_tweet','Analysis']]

#Printing unique values of Analysis variables
data['Analysis'].unique()

#Check the number of Analysis values
df['Analysis'].nunique()

#Data Visualization of Analysis Variables
# Plotting the distribution for dataset.
ax = df.groupby('Analysis').count().plot(kind='bar', title='Distribution of data',legend=False)
ax.set_xticklabels(['Negative','Neutral','Positive'], rotation=0)
# Storing data in lists.
text, sentiment = list(df['clean_tweet']), list(df['Analysis'])

import seaborn as sns
sns.countplot(x='Analysis', data=df)

#Separating positive and negative tweets
data_pos = data[data['Analysis'] == 1]
data_neu=data[data['Analysis']==0]
data_neg = data[data['Analysis'] == -1]

#Combining positive, neutral and negative tweets
dataset = pd.concat([data_pos,data_neu, data_neg])

# Ratio of positive, negative and neutral tweets
# extract the column you want to convert into a list
clean_tweet_list = df['clean_tweet'].tolist()

# Initialize counters
positive_tweets = 0
neutral_tweets = 0
negative_tweets = 0
total_tweets = 0

# Loop through each tweet and analyze its sentiment
for tweet in clean_tweet_list:
    analysis = TextBlob(tweet)
    polarity = analysis.sentiment.polarity

    # Classify sentiment as positive, neutral, or negative
    if polarity > 0:
        positive_tweets += 1
    elif polarity == 0:
        neutral_tweets += 1
    else:
        negative_tweets += 1

    total_tweets += 1

# Calculate ratios of positive, neutral, and negative tweets
if total_tweets > 0:
    positive_ratio = positive_tweets / total_tweets
    neutral_ratio = neutral_tweets / total_tweets
    negative_ratio = negative_tweets / total_tweets
    print(f"Positive tweets ratio: {positive_ratio:.2f}")
    print(f"Neutral tweets ratio: {neutral_ratio:.2f}")
    print(f"Negative tweets ratio: {negative_ratio:.2f}")
else:
    print("No tweets found in the dataset!")

#Separating input feature and label
X=data.clean_tweet
y=data.Analysis

#Plot a cloud of words for negative tweets
data_neg = data['clean_tweet'][:]
plt.figure(figsize = (20,20))
wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,
               collocations=False).generate(" ".join(data_neg))
plt.imshow(wc)

#Plot a cloud of words for positive tweets
data_pos = data['clean_tweet'][:]
wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,
              collocations=False).generate(" ".join(data_pos))
plt.figure(figsize = (20,20))
plt.imshow(wc)

#Plot a cloud of words for neutral tweets
data_neu = data['clean_tweet'][:]
wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,
              collocations=False).generate(" ".join(data_pos))
plt.figure(figsize = (20,20))
plt.imshow(wc)

# Separating the 95% data for training data and 5% for testing data
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.05, random_state =26105111)

"""# **Transforming the Dataset Using TF-IDF Vectorizer**"""

#Fit the TF-IDF Vectorizer
vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)
vectoriser.fit(X_train)
print('No. of feature_words: ', len(vectoriser.get_feature_names_out()))

#Transform the data using TF-IDF Vectorizer
X_train = vectoriser.transform(X_train)
X_test  = vectoriser.transform(X_test)

"""# **Function for Model Evaluation**"""

def model_Evaluate(model):
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    accuracy = np.trace(cm) / float(np.sum(cm))
    misclass = 1 - accuracy
    class_report = classification_report(y_test, y_pred)
    print("Confusion Matrix:")
    print(cm)
    print("Accuracy:", accuracy)
    print("Misclassification Rate:", misclass)
    print("Classification Report:", "\n", class_report)
    
    # Check the shape of the confusion matrix and adjust the code accordingly
    if cm.shape[0] == 2:
        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
        tpr = tp / (tp + fn)
        tnr = tn / (tn + fp)
        fpr = fp / (fp + tn)
        fnr = fn / (fn + tp)
        print("True Positives:", tp)
        print("True Negatives:", tn)
        print("False Positives:", fp)
        print("False Negatives:", fn)
        print("True Positive Rate (Sensitivity):", tpr)
        print("True Negative Rate (Specificity):", tnr)
        print("False Positive Rate:", fpr)
        print("False Negative Rate:", fnr)
    else:
        tn, fp, fn, tp, _, _, _, _, _ = confusion_matrix(y_test, y_pred).ravel()
        tpr = tp / (tp + fn)
        tnr = tn / (tn + fp)
        fpr = fp / (fp + tn)
        fnr = fn / (fn + tp)
        print("True Positives:", tp)
        print("True Negatives:", tn)
        print("False Positives:", fp)
        print("False Negatives:", fn)
        print("True Positive Rate (Sensitivity):", tpr)
        print("True Negative Rate (Specificity):", tnr)
        print("False Positive Rate:", fpr)
        print("False Negative Rate:", fnr)
        
    df_cm = pd.DataFrame(cm, index=["Negative", "Neutral", "Positive"], columns=["Negative", "Neutral", "Positive"])
    plt.figure(figsize=(5, 4))
    sns.heatmap(df_cm, annot=True, cmap='Blues', fmt='g')
    plt.title('Logistic Regression \nAccuracy:{0:.3f}'.format(accuracy))
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

"""# **Model Building**"""

#Logistic Regression
LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)
LRmodel.fit(X_train, y_train)


model_Evaluate(LRmodel)
y_pred3 = LRmodel.predict(X_test)

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from itertools import cycle

# Binarize the labels
y_test_bin = label_binarize(y_test, classes=[0, 1, 2])
n_classes = y_test_bin.shape[1]

# Make predictions on the test data
y_pred_prob = LRmodel.predict_proba(X_test)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot ROC curve for each class
plt.figure()
lw = 2
colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=lw, label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))

# Plot micro-average ROC curve
plt.plot(fpr["micro"], tpr["micro"], color='deeppink', lw=lw, linestyle='--', label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc["micro"]))

# Plot chance line
plt.plot([0, 1], [0, 1], 'k--', lw=lw)

# Set plot properties
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()